````markdown
![WFGY Logo](docs/logo.png)

# WFGY OS · TXT Build **Beta**

*An open-text operating scaffold—fork it, shape it, call it yours.*

> **This page is continuously updated.**  
> Last update: **2025-07-02** (Beta launch)  
> Check back regularly for the latest docs and links.

---

## Key Points

| Feature | Why it matters |
|---------|----------------|
| **Plain-text only** | No executables, no network calls, zero malware risk |
| **Semantic Tree memory** | Stores reasoning nodes—not chat logs |
| **ΔS + BBCR guard** | Detects semantic turbulence; self-corrects before hallucinating |
| **Four core modules** | `BBMC  BBPF  BBCR  BBAM` govern residue, progression, correction, attention |
| **MIT-licensed & forkable** | Copy the file, edit the language, publish your edition—fully auditable |

---

## Quick Start (three steps)

1. **Download** `HelloWorld.txt` from the `/OS` folder.  
2. **Upload / paste** the file into *any* LLM chat (ChatGPT, Claude, local model, etc.).  
3. **Type** `hello world` (case-insensitive).  
   You’ll see the language screen—select a language and the OS boots.

> **Tested platforms (100 % OK so far)**  
> ChatGPT (o 3 / o 4o) • Claude-3 Opus • Phi-3-mini  
> *More platforms will be listed as they are verified; absence here means “untested”, not “unsupported”. Beta feedback is welcome—open an issue or discussion.*

---

### Console Basics

```txt
Start           load memory + boundary guard + menus
build / 1.1     create a semantic node
view  / 1.2     list recent nodes
kbtest          watch the knowledge-boundary guard
menu            return to the short main console
logo            display the TXT logo
````

---

## Roadmap

| Date (UTC)     | Milestone                                             |
| -------------- | ----------------------------------------------------- |
| **2025-07-02** | **Beta** (this build) — Zenodo DOI release            |
| **2025-07-07** | **v 1.0** — cross-platform tweaks & packaged TXT apps |

TXT apps are also plain text; the term *app* is for clarity only.

---

## Contributing & App Hub

* **Fork** this repo and create your own `.txt` OS or app.
* **Upload** finished apps to the **WFGY Zenodo community** (link drops at v 1.0).
* A GPT-4o script checks license, ASCII-only, and safety guidelines.
* Curated entries will appear under **`/apps`** in this repository.

---

## Repository Layout

```text
/OS        core TXT builds & changelogs
/apps      community TXT apps   (opens 2025-07-07)
/docs      white paper & diagrams
```

Primary entry: [https://github.com/onestardao/WFGY](https://github.com/onestardao/WFGY)
Direct OS updates: [https://github.com/onestardao/WFGY/tree/main/OS](https://github.com/onestardao/WFGY/tree/main/OS)

> **No auto-update:** always download the newest TXT manually.

---

## License

> MIT License — © 2025 The WFGY Project

---

## Hidden Tip

> Type `logo` inside the console to view the TXT logo.

---

## FAQ

### 1 · How does WFGY give AI memory?

It detects semantic jumps (high ΔS) and writes nodes to a **Semantic Tree**. Each node stores topic, module, and tension—creating a recoverable reasoning path.

### 2 · What is ΔS, and how does it prevent hallucination?

ΔS measures semantic tension. When it exceeds a safe threshold, the **BBCR** module reroutes logic or asks for confirmation, stopping confident nonsense.

### 3 · How can a single TXT file achieve this?

All logic, boundary checks, and memory rules are encoded in natural language. The AI reads and obeys them—no code is executed.

### 4 · Why call it an OS, not a prompt?

It manages AI memory, logic, and boundaries—just as an operating system manages processes. You can reboot, patch, or extend it with plain text.

### 5 · What do the four core modules do?

* **BBMC** – minimise semantic residue
* **BBPF** – multi-path logical progression
* **BBCR** – collapse–rebirth correction
* **BBAM** – attention & tone modulation

### 6 · Semantic Tree vs standard memory—can it recover forgotten info?

Standard memory stores text snippets. The Semantic Tree stores logical context, so reasoning can be reconstructed even after tokens drop.

### 7 · How does the BBMC formula improve reasoning?

`B = I - G + m*c^2` quantifies deviation from ground truth, enabling the model to self-correct across turns.

### 8 · How can I verify WFGY isn’t fake?

Paste the TXT into any LLM and run `kbtest` or ask *“how does memory work?”*. The AI will answer using the embedded logic.

### 9 · Can WFGY integrate with agents or workflows?

Yes. Load the TXT as the reasoning core, then layer external tools or APIs on top.

### 10 · Commercial use and license?

MIT—free for commercial or personal projects; keep the copyright and disclaimer.

### 11 · How do I fork or customise WFGY?

Copy `HelloWorld.txt`, edit the semantic rules, rename the file, and publish. AI will follow your structure as long as it’s coherent.

```
```
